{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "\n",
    "# Define command line arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, help='Path to dataset ')\n",
    "parser.add_argument('--gpu', action='store_true', help='Use GPU if available')\n",
    "parser.add_argument('--epochs', type=int, help='Number of epochs')\n",
    "parser.add_argument('--arch', type=str, help='Model architecture')\n",
    "parser.add_argument('--learning_rate', type=float, help='Learning rate')\n",
    "parser.add_argument('--hidden_units', type=int, help='Number of hidden units')\n",
    "parser.add_argument('--checkpoint', type=str, help='Save trained model checkpoint to file')\n",
    "\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# This method loads and tunes in a model\n",
    "def load_model(arch='vgg19', num_labels=102, hidden_units=4096):\n",
    "    # Load a pre-trained model\n",
    "    if arch=='vgg19':\n",
    "        # Load a pre-trained model\n",
    "        model = models.vgg19(pretrained=True)\n",
    "    elif arch=='alexnet':\n",
    "        model = models.alexnet(pretrained=True)\n",
    "    else:\n",
    "        raise ValueError('Unexpected network architecture', arch)\n",
    "        \n",
    "    # Freeze its parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Features, removing the last layer\n",
    "    features = list(model.classifier.children())[:-1]\n",
    "  \n",
    "    # Number of filters in the bottleneck layer\n",
    "    num_filters = model.classifier[len(features)].in_features\n",
    "\n",
    "    # Extend the existing architecture with new layers\n",
    "    features.extend([\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(num_filters, hidden_units),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(hidden_units, hidden_units),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(hidden_units, num_labels),\n",
    "        ##nn.Softmax(dim=1) \n",
    "        # Please, notice Softmax layer has not been added as per Pytorch answer:\n",
    "        # https://github.com/pytorch/vision/issues/432#issuecomment-368330817\n",
    "        # It is not either included in its transfer learning tutorial:\n",
    "        # https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "    ])\n",
    "    \n",
    "    model.classifier = nn.Sequential(*features)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# This method trains a model\n",
    "def train_model(image_datasets, arch='vgg19', hidden_units=4096, epochs=25, learning_rate=0.001, gpu=False, checkpoint=''):\n",
    "    # Use command line values when specified\n",
    "    if args.arch:\n",
    "        arch = args.arch     \n",
    "        \n",
    "    if args.hidden_units:\n",
    "        hidden_units = args.hidden_units\n",
    "\n",
    "    if args.epochs:\n",
    "        epochs = args.epochs\n",
    "            \n",
    "    if args.learning_rate:\n",
    "        learning_rate = args.learning_rate\n",
    "\n",
    "    if args.gpu:\n",
    "        gpu = args.gpu\n",
    "\n",
    "    if args.checkpoint:\n",
    "        checkpoint = args.checkpoint        \n",
    "        \n",
    "    # Using the image datasets, define the dataloaders\n",
    "    dataloaders = {\n",
    "        x: data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=2)\n",
    "        for x in list(image_datasets.keys())\n",
    "    }\n",
    " \n",
    "    # Calculate dataset sizes.\n",
    "    dataset_sizes = {\n",
    "        x: len(dataloaders[x].dataset) \n",
    "        for x in list(image_datasets.keys())\n",
    "    }    \n",
    "\n",
    "        \n",
    "    print('Network architecture:', arch)\n",
    "    print('Number of hidden units:', hidden_units)\n",
    "    print('Number of epochs:', epochs)\n",
    "    print('Learning rate:', learning_rate)\n",
    "\n",
    "    # Load the model     \n",
    "    num_labels = len(image_datasets['train'].classes)\n",
    "    model = load_model(arch=arch, num_labels=num_labels, hidden_units=hidden_units)\n",
    "\n",
    "    # Use gpu if selected and available\n",
    "    if gpu and torch.cuda.is_available():\n",
    "        print('Using GPU for training')\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print('Using CPU for training')\n",
    "        device = torch.device(\"cpu\")     \n",
    "\n",
    "                \n",
    "    # Defining criterion, optimizer and scheduler\n",
    "    # Observe that only parameters that require gradients are optimized\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(list(filter(lambda p: p.requires_grad, model.parameters())), lr=learning_rate, momentum=0.9)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)    \n",
    "        \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Store class_to_idx into a model property\n",
    "    model.class_to_idx = image_datasets['train'].class_to_idx\n",
    "    \n",
    "    # Save checkpoint if requested\n",
    "    if checkpoint:\n",
    "        print ('Saving checkpoint to:', checkpoint) \n",
    "        checkpoint_dict = {\n",
    "            'arch': arch,\n",
    "            'class_to_idx': model.class_to_idx, \n",
    "            'state_dict': model.state_dict(),\n",
    "            'hidden_units': hidden_units\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint_dict, checkpoint)\n",
    "    \n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train model if invoked from command line\n",
    "if args.data_dir:    \n",
    "    # Default transforms for the training, validation, and testing sets\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomRotation(45),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "         ]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "         ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "         ])\n",
    "    }\n",
    "    \n",
    "    # Load the datasets with ImageFolder\n",
    "    image_datasets = {\n",
    "        x: datasets.ImageFolder(root=args.data_dir + '/' + x, transform=data_transforms[x])\n",
    "        for x in list(data_transforms.keys())\n",
    "    }\n",
    "        \n",
    "    train_model(image_datasets) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
